{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/santiago-cortes14/Tesis-Pinos/blob/main/Detectron2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BsE8lxClMR2O"
      },
      "source": [
        "# 1. Installation of dependencies // Instalación de las dependencias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "betP_k_KIVjX"
      },
      "outputs": [],
      "source": [
        "# Install Pytorch\n",
        "!pip3 install -U torch==1.5 torchvision==0.6 -f https://download.pytorch.org/whl/cu101/torch_stable.html \n",
        "!pip3 install cython pyyaml==5.1\n",
        "!pip3 install -U 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'\n",
        "\n",
        "# Install Detectron2:\n",
        "!pip3 install detectron2==0.1.3 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu101/torch1.5/index.html\n",
        "\n",
        "# Install tools\n",
        "!pip3 install simplejson\n",
        "!pip3 install progressbar\n",
        "\n",
        "# Pytorch backend \n",
        "import torch, torchvision\n",
        "print(torch.__version__, torch.cuda.is_available())\n",
        "!gcc --version\n",
        "!pip3 install google-cloud-storage\n",
        "\n",
        "# Install Labelbox\n",
        "!pip3 install labelbox\n",
        "!pip install \"labelbox[data]\"\n",
        "\n",
        "# Clear output\n",
        "from IPython.display import clear_output\n",
        "clear_output()\n",
        "print(\"Dependencies installed correctly!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J01g4Nt9fKYT"
      },
      "source": [
        "# 2. Import of libraries // Importación de librerias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bbe4YYJrTXSY"
      },
      "outputs": [],
      "source": [
        "# General \n",
        "import datetime as dt\n",
        "import os, os.path\n",
        "import sys\n",
        "import random\n",
        "import time\n",
        "from itertools import cycle\n",
        "from uuid import uuid4\n",
        "import requests\n",
        "from pprint import pprint\n",
        "from multiprocessing.pool import ThreadPool\n",
        "import numpy as np\n",
        "import cv2\n",
        "from skimage import io\n",
        "import simplejson as json\n",
        "from datetime import datetime\n",
        "import time\n",
        "import shutil\n",
        "from matplotlib import pyplot as plt\n",
        "from pycocotools import mask\n",
        "import progressbar\n",
        "from PIL import Image\n",
        "from google.cloud import storage\n",
        "from google.colab.patches import cv2_imshow\n",
        "import copy\n",
        "import torch\n",
        "\n",
        "# Labelbox \n",
        "import labelbox as lb\n",
        "from labelbox import Project, Dataset,Client, OntologyBuilder\n",
        "from labelbox.schema.bulk_import_request import BulkImportRequest\n",
        "from labelbox.schema.enums import BulkImportRequestState\n",
        "\n",
        "# Detectron2 \n",
        "from detectron2.structures import BoxMode\n",
        "from detectron2.engine import DefaultTrainer\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.data import DatasetCatalog, MetadataCatalog\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.evaluation import COCOEvaluator\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.utils.visualizer import ColorMode\n",
        "from detectron2.data import detection_utils as utils\n",
        "import detectron2.data.transforms as T\n",
        "\n",
        "clear_output()\n",
        "print(\"Libraries imported correctly!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eeP39WxOMpkV"
      },
      "source": [
        "# 3. Parameter declaration // Declaración de parámetros"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k7c5qs1COhy3"
      },
      "outputs": [],
      "source": [
        "# Linking our dataset with labelbox\n",
        "\n",
        "LB_API_KEY ='eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1c2VySWQiOiJja3d3aDI3Z2swa3M0MHo5ZzE1eTBiNXlvIiwib3JnYW5pemF0aW9uSWQiOiJja3czenVkOHEwemluMHpiNzh5NTNhdWZtIiwiYXBpS2V5SWQiOiJjbDByZDBwancwZ202MHo2NDdycmFibGkxIiwic2VjcmV0IjoiNmNkY2MyYzQ2ZWRlNTNlY2E2N2EzZjkyZTRiMmQ2NTYiLCJpYXQiOjE2NDczMDE5MDAsImV4cCI6MjI3ODQ1MzkwMH0.B6UY9SMoH9MFcGSWcCyKHdyznuo4IGe39vbH4j6vzyo'\n",
        "PROJECT_ID='ckzm36rbndjk60z781xkcg9an' # Labelbox project id\n",
        "DATASETS=['ckzm39bj003dp0z9d422q9y4s'] # Labelbox dataset ids attached to the project\n",
        "MODE = 'object-detection' \n",
        "DATA_LOCATION = 'obj-data'\n",
        "\n",
        "# Configuration of Detectron2 parameters\n",
        "\n",
        "DOWNLOAD_IMAGES = True  # Download data from labelbox.\n",
        "TRAIN_RATIO = 0.6       # Training data\n",
        "VAL_TEST_RATIO = 0.2    # Validation data / Test data\n",
        "NUM_CPU_THREADS = 8     # For multiprocess downloads\n",
        "NUM_SAMPLE_LABELS = 0 \n",
        "PRELABELING_THRESHOLD = 0.6 # Minimum model inference confidence threshold to be uploaded to labelbox\n",
        "HEADLESS_MODE = False # Set True to skip previewing data or model results\n",
        "\n",
        "DETECTRON_DATASET_TRAINING_NAME = 'prelabeling-train'\n",
        "DETECTRON_DATASET_VALIDATION_NAME = 'prelabeling-val'\n",
        "DETECTRON_DATASET_TEST_NAME = 'prelabeling-test'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttsWdnXeMw8O"
      },
      "source": [
        "# 4. Labelbox request // Conexión entre Labelbox y Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LI5Q2j0WQAh6"
      },
      "outputs": [],
      "source": [
        "# Get project ontology from labelbox\n",
        "\n",
        "def get_ontology(project_id):\n",
        "    response = client.execute(\n",
        "                \"\"\"\n",
        "                query getOntology (\n",
        "                    $project_id : ID!){ \n",
        "                    project (where: { id: $project_id }) { \n",
        "                        ontology { \n",
        "                            normalized \n",
        "                        } \n",
        "                    }\n",
        "                }\n",
        "                \"\"\",\n",
        "                {\"project_id\": project_id})\n",
        "            \n",
        "    ontology = response['project']['ontology']['normalized']['tools']\n",
        "\n",
        "    # Return list of tools and embed category id to be used to map classname during training and inference\n",
        "    \n",
        "    mapped_ontology = []\n",
        "    thing_classes = []\n",
        "    \n",
        "    i=0\n",
        "    for item in ontology:\n",
        "        item.update({'category': i})\n",
        "        mapped_ontology.append(item)\n",
        "        thing_classes.append(item['name'])\n",
        "        i=i+1\n",
        "\n",
        "    return mapped_ontology, thing_classes\n",
        "\n",
        "# Creates a new export request to get all labels from labelbox. \n",
        "\n",
        "def get_labels(project_id):\n",
        "    should_poll = 1\n",
        "    while(should_poll == 1):\n",
        "        response = client.execute(\n",
        "                    \"\"\"\n",
        "                    mutation export(\n",
        "                    $project_id : ID!    \n",
        "                    )\n",
        "                    { \n",
        "                        exportLabels(data:{ projectId: $project_id }){ \n",
        "                            downloadUrl \n",
        "                            createdAt \n",
        "                            shouldPoll \n",
        "                        }\n",
        "                    }\n",
        "                    \"\"\",\n",
        "                    {\"project_id\": project_id})\n",
        "        \n",
        "        if response['exportLabels']['shouldPoll'] == False:\n",
        "            should_poll = 0\n",
        "            url = response['exportLabels']['downloadUrl']\n",
        "            headers = {\"User-Agent\":\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36\"}\n",
        "\n",
        "            r = requests.get(url, headers=headers)\n",
        "            \n",
        "            print('Export generated')\n",
        "            \n",
        "            # Writing export to disc for easier debugging\n",
        "            \n",
        "            open('export.json', 'wb').write(r.content)\n",
        "            return r.content\n",
        "        else:\n",
        "            print('Waiting for export generation. Will check back in 10 seconds.')    \n",
        "            time.sleep(10)\n",
        "\n",
        "    return response\n",
        "\n",
        "# Get all previous predictions import (bulk import request). \n",
        "\n",
        "def get_current_import_requests():\n",
        "    response = client.execute(\n",
        "                    \"\"\"\n",
        "                    query get_all_import_requests(\n",
        "                        $project_id : ID! \n",
        "                    ) {\n",
        "                      bulkImportRequests(where: {projectId: $project_id}) {\n",
        "                        id\n",
        "                        name\n",
        "                      }\n",
        "                    }\n",
        "                    \"\"\",\n",
        "                    {\"project_id\": PROJECT_ID})\n",
        "    \n",
        "    return response['bulkImportRequests']\n",
        "\n",
        "# Delete all current predictions in a project and dataset. We want to delete them and start fresh with predictions from the latest model iteration\n",
        "\n",
        "def delete_import_request(import_request_id):\n",
        "    response = client.execute(\n",
        "                    \"\"\"\n",
        "                        mutation delete_import_request(\n",
        "                            $import_request_id : ID! \n",
        "                        ){\n",
        "                          deleteBulkImportRequest(where: {id: $import_request_id}) {\n",
        "                            id\n",
        "                            name\n",
        "                          }\n",
        "                        }\n",
        "                    \"\"\",\n",
        "                    {\"import_request_id\": import_request_id})\n",
        "    \n",
        "    return response\n",
        "\n",
        "# Function to return the difference between two lists. This is used to compute the queued datarows to be used for inference. \n",
        "\n",
        "def diff_lists(li1, li2): \n",
        "    li_dif = [i for i in li1 + li2 if i not in li1 or i not in li2] \n",
        "    return li_dif \n",
        "\n",
        "# Generic data download function\n",
        "\n",
        "def download_files(filemap):\n",
        "    path, uri = filemap    \n",
        "    \n",
        "    # Download data\n",
        "    \n",
        "    if not os.path.exists(path):\n",
        "        r = requests.get(uri, stream=True)\n",
        "        if r.status_code == 200:\n",
        "            with open(path, 'wb') as f:\n",
        "                for chunk in r:\n",
        "                    f.write(chunk)\n",
        "    return path\n",
        "\n",
        "#____________________________________________ CONVERT BINARY IMAGE INTO COCO RLE FORMAT_____________________________________________________________________\n",
        "\n",
        "def rle_encode(mask_image):\n",
        "    size = list(mask_image.shape)\n",
        "    pixels = mask_image.flatten()\n",
        "    \n",
        "    pixels[0] = 0\n",
        "    pixels[-1] = 0\n",
        "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 2\n",
        "    runs[1::2] = runs[1::2] - runs[:-1:2]\n",
        "    \n",
        "    rle = {'counts': runs.tolist(), 'size': size}\n",
        "    return rle\n",
        "\n",
        "def load_set(dir):\n",
        "    with open(dir+\"dataset.json\") as json_file:\n",
        "        dataset_dicts = json.loads(json_file)\n",
        "    return dataset_dicts\n",
        "\n",
        "def cv2_imshow(a, **kwargs):\n",
        "\n",
        "    # Cv2 stores colors as BGR; convert to RGB\n",
        "    \n",
        "    if a.ndim == 3:\n",
        "        if a.shape[2] == 4:\n",
        "            a = cv2.cvtColor(a, cv2.COLOR_BGRA2RGBA)\n",
        "        else:\n",
        "            a = cv2.cvtColor(a, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    return plt.imshow(a, **kwargs)\n",
        "\n",
        "def upload_to_gcs(file_name):\n",
        "    bucket = storage_client.get_bucket(\"predictions-import-test\")\n",
        "    blob = bucket.blob(\"{}.png\".format(str(uuid4())))\n",
        "    blob.upload_from_filename(file_name)\n",
        "    return blob.generate_signed_url(dt.timedelta(weeks=10))\n",
        "\n",
        "def mask_to_cloud(img, mask_array, filename):\n",
        "    num_instances = mask_array.shape[0]\n",
        "    mask_array = np.moveaxis(mask_array, 0, -1)\n",
        "    mask_array_instance = []\n",
        "    output = np.zeros_like(img)\n",
        "    for i in range(num_instances):\n",
        "        mask_array_instance.append(mask_array[:, :, i:(i+1)])\n",
        "        output = np.where(mask_array_instance[i] == True, 255, output)\n",
        "    im = Image.fromarray(output)\n",
        "    im.save(DATA_LOCATION+'tmp/'+filename+'.png')\n",
        "    \n",
        "    cloud_mask = upload_to_gcs(DATA_LOCATION+'tmp/'+filename+'.png')\n",
        "    \n",
        "    return cloud_mask\n",
        "\n",
        "#________________________________________________CONVERT LABELBOX LABELS INTO DETECTRON2 FORMAT___________________________________________________________________\n",
        "\n",
        "def load_detectron2_dataset(labels, ontology, thing_classes, dir):\n",
        "    dataset_dicts = []\n",
        "    i = 0\n",
        "    total = len(labels)\n",
        "\n",
        "    print(\"Num labels processing: \" + str(total))\n",
        "    time.sleep(1)\n",
        "    bar = progressbar.ProgressBar(maxval=total, \\\n",
        "        widgets=[progressbar.Bar('=', '[', ']'), ' ', progressbar.Percentage()])\n",
        "    bar.start()\n",
        " \n",
        "    # Write detectron2 dataset file to disk for easier debugging\n",
        "\n",
        "    for label in labels:\n",
        "        \n",
        "        try:\n",
        "            record = {}\n",
        "            filename = os.path.join(dir, label['External ID'])\n",
        "            \n",
        "            _ = io.imread(filename)\n",
        "            \n",
        "            height, width = cv2.imread(filename).shape[:2]\n",
        "\n",
        "            record[\"file_name\"] = filename\n",
        "            record[\"height\"] = height\n",
        "            record[\"width\"] = width\n",
        "            record[\"image_id\"] = label['ID']\n",
        "\n",
        "            objs = []\n",
        "\n",
        "            for instance in label['Label']['objects']:\n",
        "                category_id = thing_classes.index(instance['title'])\n",
        "                \n",
        "                if MODE == 'object-detection':\n",
        "                    obj = {\n",
        "                            \"bbox\": [instance['bbox']['left'], instance['bbox']['top'], instance['bbox']['width'], instance['bbox']['height']],\n",
        "                            \"bbox_mode\": BoxMode.XYWH_ABS,\n",
        "                            \"segmentation\": [],\n",
        "                            \"category_id\": category_id,\n",
        "                        }\n",
        "                    objs.append(obj)\n",
        "\n",
        "                if MODE == 'segmentation-rle':\n",
        "                    path = DATA_LOCATION+masks+'/'+label['External ID']\n",
        "                    mask_URI = instance['instanceURI']\n",
        "                    downloaded_path = download_files((path, mask_URI))\n",
        "                    im = cv2.imread(downloaded_path,0)\n",
        "\n",
        "                    binary = np.array(im)\n",
        "\n",
        "                    rle = mask.encode(np.asfortranarray(binary))\n",
        "                    ground_truth_bounding_box = mask.toBbox(rle)\n",
        "\n",
        "                    obj = {\n",
        "                            \"bbox\": ground_truth_bounding_box.tolist(),\n",
        "                            \"bbox_mode\": BoxMode.XYWH_ABS,\n",
        "                            \"segmentation\": rle,\n",
        "                            \"category_id\": category_id,\n",
        "                            \"iscrowd\": 0\n",
        "                        }\n",
        "                    objs.append(obj)\n",
        "\n",
        "            record[\"annotations\"] = objs\n",
        "            dataset_dicts.append(record)\n",
        "            \n",
        "            bar.update(i+1)\n",
        "            i=i+1\n",
        "        except Exception as e:\n",
        "            print('Exception: ', e)\n",
        "\n",
        "    bar.finish()\n",
        "    f = open(dir+\"dataset_dict.json\",\"w\")\n",
        "    f.write(json.dumps(dataset_dicts))\n",
        "    f.close()\n",
        "    \n",
        "    # Write detectron2 dataset file to disk for easier debugging\n",
        "    \n",
        "    return dataset_dicts\n",
        "\n",
        "class CocoTrainer(DefaultTrainer):\n",
        "\n",
        "    @classmethod\n",
        "    def build_evaluator(cls, cfg, dataset_name, output_folder=None):\n",
        "\n",
        "        if output_folder is None:\n",
        "            os.makedirs(\"coco_eval\", exist_ok=True)\n",
        "            output_folder = \"coco_eval\"\n",
        "\n",
        "        return COCOEvaluator(dataset_name, cfg, False, output_folder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AL93yUfDNwxC"
      },
      "source": [
        "# 5. Declaration of pre-trained model // Declaración del modelo pre-entrenado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YCdtzAwLQL5E"
      },
      "outputs": [],
      "source": [
        "start_time = time.time()\n",
        "\n",
        "if os.path.exists('coco_eval'):\n",
        "    shutil.rmtree('coco_eval')\n",
        "    \n",
        "client = lb.Client(LB_API_KEY, \"https://api.labelbox.com/graphql\")\n",
        "storage_client = storage.Client()\n",
        "\n",
        "# Get labelbox project\n",
        "\n",
        "project = client.get_project(PROJECT_ID)\n",
        "\n",
        "# Get ontology\n",
        "\n",
        "ontology, thing_classes = get_ontology(PROJECT_ID)\n",
        "print('Available classes: ', thing_classes)\n",
        "\n",
        "# Get labels\n",
        "\n",
        "labels = json.loads(get_labels(PROJECT_ID))\n",
        "\n",
        "# Split training and validation labels\n",
        "\n",
        "if NUM_SAMPLE_LABELS !=0:\n",
        "    val_sample = int(VAL_TEST_RATIO*NUM_SAMPLE_LABELS)\n",
        "    val_labels = random.sample(labels, val_sample)\n",
        "    test_sample = int(VAL_TEST_RATIO*NUM_SAMPLE_LABELS)\n",
        "    test_labels = random.sample(labels, test_sample)\n",
        "    train_labels = random.sample(labels, NUM_SAMPLE_LABELS)\n",
        "else:\n",
        "    split = int(VAL_TEST_RATIO*len(labels))\n",
        "    val_labels = labels[:split]\n",
        "    test_labels = labels[split:split*2]\n",
        "    train_labels = labels[split*2:]\n",
        "\n",
        "# Check and create folders for downloading data from Labelbox\n",
        "\n",
        "train = 'train'\n",
        "val = 'val'\n",
        "test='test'\n",
        "\n",
        "inference = 'inference'\n",
        "masks = 'masks'\n",
        "tmp = 'tmp'\n",
        "output = 'out'\n",
        "\n",
        "if not os.path.exists(DATA_LOCATION):\n",
        "    os.makedirs(DATA_LOCATION)\n",
        "\n",
        "if not os.path.exists(DATA_LOCATION+train):\n",
        "    os.makedirs(DATA_LOCATION+train)\n",
        "    \n",
        "if not os.path.exists(DATA_LOCATION+val):\n",
        "    os.makedirs(DATA_LOCATION+val)\n",
        "\n",
        "if not os.path.exists(DATA_LOCATION+test):\n",
        "    os.makedirs(DATA_LOCATION+test)\n",
        "    \n",
        "if not os.path.exists(DATA_LOCATION+tmp):\n",
        "    os.makedirs(DATA_LOCATION+tmp)\n",
        "\n",
        "if not os.path.exists(DATA_LOCATION+output):\n",
        "    os.makedirs(DATA_LOCATION+output)\n",
        "\n",
        "# Download training and validation labels in parallel\n",
        "\n",
        "train_urls = []\n",
        "for label in train_labels:\n",
        "    train_urls.append((DATA_LOCATION+'train/' + label['External ID'], label['Labeled Data']))\n",
        "\n",
        "val_urls = []\n",
        "for label in val_labels:\n",
        "    val_urls.append((DATA_LOCATION+'val/' + label['External ID'], label['Labeled Data']))\n",
        "\n",
        "test_urls = []\n",
        "for label in test_labels:\n",
        "    test_urls.append((DATA_LOCATION+'test/' + label['External ID'], label['Labeled Data']))\n",
        "\n",
        "if(DOWNLOAD_IMAGES==True):\n",
        "    print('Downloading training and validation data... \\n')\n",
        "    \n",
        "    results_train = ThreadPool(NUM_CPU_THREADS).imap_unordered(download_files, train_urls)\n",
        "    results_val = ThreadPool(NUM_CPU_THREADS).imap_unordered(download_files, val_urls)\n",
        "    results_test = ThreadPool(NUM_CPU_THREADS).imap_unordered(download_files, test_urls)\n",
        "\n",
        "    for item in results_train:\n",
        "        pass\n",
        "    for item in results_val:\n",
        "        pass\n",
        "    for item in results_test:\n",
        "        pass\n",
        "    \n",
        "    print('Finished downloading training and validation data... \\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HK-sEHFQHkyZ"
      },
      "source": [
        "## Preparing the database for detectron2 // Preparar la base de datos para Detectron2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Ta05hSxSctO"
      },
      "outputs": [],
      "source": [
        "# Load dataset into Detectron2 prelabeling\n",
        "\n",
        "try:\n",
        "    DatasetCatalog.register(DETECTRON_DATASET_TRAINING_NAME, lambda: load_detectron2_dataset(train_labels, ontology, thing_classes, DATA_LOCATION+'train/' ))\n",
        "    DatasetCatalog.register(DETECTRON_DATASET_VALIDATION_NAME, lambda: load_detectron2_dataset(val_labels, ontology, thing_classes, DATA_LOCATION+'val/' ))\n",
        "    DatasetCatalog.register(DETECTRON_DATASET_TEST_NAME, lambda: load_detectron2_dataset(test_labels, ontology, thing_classes, DATA_LOCATION+'test/' ))\n",
        "\n",
        "    MetadataCatalog.get(DETECTRON_DATASET_TRAINING_NAME).thing_classes=thing_classes\n",
        "    MetadataCatalog.get(DETECTRON_DATASET_VALIDATION_NAME).thing_classes=thing_classes\n",
        "    MetadataCatalog.get(DETECTRON_DATASET_TEST_NAME).thing_classes=thing_classes\n",
        "\n",
        "except Exception as e:\n",
        "    print(e)\n",
        "\n",
        "if MODE == 'object-detection':\n",
        "     model = 'COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml'\n",
        "\n",
        "if MODE == 'segmentation-rle':\n",
        "     model = 'COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml'\n",
        "\n",
        "# Load data and metadata for visualization and inference\n",
        "\n",
        "dataset_dicts = DatasetCatalog.get(DETECTRON_DATASET_TRAINING_NAME)\n",
        "dataset_dicts_val = DatasetCatalog.get(DETECTRON_DATASET_VALIDATION_NAME)\n",
        "metadata = MetadataCatalog.get(DETECTRON_DATASET_TRAINING_NAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RyUYq084iC-M"
      },
      "source": [
        "# 6. Train model configs // Configuraciones de entrenamiento del modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qkLBWUEslWwM"
      },
      "outputs": [],
      "source": [
        "# Train the model. Change the parameters as per your needs. \n",
        "    \n",
        "cfg = get_cfg()\n",
        "\n",
        "cfg.merge_from_file(model_zoo.get_config_file(model))\n",
        "cfg.DATASETS.TRAIN = (DETECTRON_DATASET_TRAINING_NAME,)\n",
        "cfg.DATASETS.TEST = (DETECTRON_DATASET_VALIDATION_NAME,)   \n",
        "cfg.DATASETS.VAL = (DETECTRON_DATASET_TEST_NAME,)  \n",
        "\n",
        "cfg.TEST.EVAL_PERIOD = 150\n",
        "cfg.DATALOADER.NUM_WORKERS = 4\n",
        "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(model)\n",
        "cfg.SOLVER.IMS_PER_BATCH = 8\n",
        "\n",
        "cfg.SOLVER.BASE_LR = 0.00125\n",
        "cfg.SOLVER.MAX_ITER = 1500\n",
        "\n",
        "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 256   \n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = len(thing_classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train and validation of model // Entrenamiento y validación del modelo"
      ],
      "metadata": {
        "id": "6FLVxP_ZYgoU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4-5jS8tSI1-Q"
      },
      "outputs": [],
      "source": [
        "if MODE=='segmentation-rle':\n",
        "    cfg.INPUT.MASK_FORMAT='bitmask'\n",
        "\n",
        "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
        "trainer = CocoTrainer(cfg) \n",
        "trainer.resume_or_load(resume=False)\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gkNxzV4mEtT"
      },
      "source": [
        "# 7. Creation of our Detectron2 model // Creación de nuestro modelo del Detectron2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vqcq7faFdoLv"
      },
      "outputs": [],
      "source": [
        "# Set newly trained model for inference. Make sure to set the appropriate threshold. \n",
        "\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = PRELABELING_THRESHOLD  # set threshold for this model\n",
        "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n",
        "\n",
        "# Create predictor\n",
        "predictor = DefaultPredictor(cfg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHzIm1CPmLF5"
      },
      "source": [
        "# 8. Preview inferences // Pre-visualización de inferencias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vMLTH062iSvq"
      },
      "outputs": [],
      "source": [
        " dataset_dicts_test = DatasetCatalog.get(DETECTRON_DATASET_TEST_NAME)\n",
        "\n",
        "if HEADLESS_MODE==False:\n",
        "    for d in random.sample(dataset_dicts_test, len(dataset_dicts_test)):    \n",
        "            im = cv2.imread(d[\"file_name\"])\n",
        "            outputs = predictor(im)\n",
        "            categories = outputs[\"instances\"].to(\"cpu\").pred_classes.numpy()\n",
        "            predicted_boxes = outputs[\"instances\"].to(\"cpu\").pred_boxes\n",
        "            \n",
        "            if MODE=='segmentation-rle':\n",
        "                pred_masks = outputs[\"instances\"].to(\"cpu\").pred_masks.numpy()\n",
        "\n",
        "            if len(categories) != 0:\n",
        "                for i in range(len(categories)):\n",
        "                    classname = thing_classes[categories[i]]\n",
        "                    for item in ontology:\n",
        "                        if classname==item['name']:\n",
        "                            schema_id = item['featureSchemaId']\n",
        "\n",
        "            v = Visualizer(im[:, :, ::-1],\n",
        "                        metadata=metadata, \n",
        "                        #instance_mode=ColorMode.IMAGE_BW,\n",
        "                        scale=2,\n",
        "            )\n",
        "            v = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
        "          \n",
        "            plt.rcParams['figure.figsize'] = (24, 48)\n",
        "            plt.imsave(os.path.join('/content/obj-dataout/', d[\"file_name\"][13:]),v.get_image()[:, :, ::-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wT5EuhzjSS96"
      },
      "source": [
        "# 9. Results // Resultados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WQ9YaMVy4AGi"
      },
      "outputs": [],
      "source": [
        "from torch import distributed\n",
        "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
        "from detectron2.data import build_detection_test_loader\n",
        "evaluator = COCOEvaluator(\"prelabeling-test\",cfg,distributed, output_dir=\"./obj-datatest\")\n",
        "val_loader = build_detection_test_loader(cfg, \"prelabeling-test\")\n",
        "print(inference_on_dataset(predictor.model, val_loader, evaluator))\n",
        "# another equivalent way to evaluate the model is to use `trainer.test"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Display of results on Tensorboard // Visualización de los resultados en Tensorboard"
      ],
      "metadata": {
        "id": "L5KCiEijaG9o"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z-ZPEziJ7g4I"
      },
      "outputs": [],
      "source": [
        "# Look at training curves in tensorboard:\n",
        "\n",
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHX3QQn44R9l"
      },
      "source": [
        "\n",
        "# 10. References // Referencias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xkkNmeaQ4NMK"
      },
      "outputs": [],
      "source": [
        "# @misc{wu2019detectron2,\n",
        "#   author =       {Yuxin Wu and Alexander Kirillov and Francisco Massa and\n",
        "#                   Wan-Yen Lo and Ross Girshick},\n",
        "#   title =        {Detectron2},\n",
        "#   howpublished = {\\url{https://github.com/facebookresearch/detectron2}},\n",
        "#   year =         {2019}\n",
        "# }"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "eeP39WxOMpkV"
      ],
      "machine_shape": "hm",
      "name": "Detectron2.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}